{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.Summary\"></a>\n",
    "# 1.Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.kaggle.com/c/google-quest-challenge\n",
    "- BERT https://zhuanlan.zhihu.com/p/269516862"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1.EDA\n",
    "- 2.Preprocess data\n",
    "- 3.Classify\n",
    "- 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, we have a Q&A dataset and there are 30+ columns `[0,1]` data coming from human evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.Table of Contents\"></a>\n",
    "# 2.Tabole of Contents\n",
    "<a href=\"#1.Summary\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li><a href=\"#1.Summary\">Summary</a></li>\n",
    "    <li><a href=\"#2.Tabole of Contents\">Tabole of Contents</a></li>\n",
    "    <li><a href=\"#3.Import Packages\">Import Packages</a></li>\n",
    "    <li><a href=\"#4.Preprocess\">Preprocess</a></li>\n",
    "    <ul>\n",
    "       <li><a href=\"#4.1 Import Data\">4.1 Import Data</a></li>\n",
    "       <li><a href=\"#4.2 Clean Data\">4.2 Clean Data</a></li> \n",
    "    </ul>\n",
    "    <li><a href=\"#5.EDA\">EDA</a></li>\n",
    "    <ul>\n",
    "       <li><a href=\"#5.1 Model_A\">5.1 Model_A</a></li>\n",
    "       <li><a href=\"#5.2 Model_B\">5.2 Model_B</a></li> \n",
    "       <li><a href=\"#5.3 Model_C\">5.3 Model_C</a></li>\n",
    "       <li><a href=\"#5.4 Split and Fit\">5.4 Split and Fit</a></li>\n",
    "       <li><a href=\"#5.5 Main_Function\">5.5 Main_Function</a></li> \n",
    "       <li><a href=\"#5.6 Plot and Analysis\">5.6 Plot and Analysis</a></li> \n",
    "    </ul>\n",
    "    <li><a href=\"#6.Task 3: Fight overfitting\">Task 3: Fight overfitting</a></li>\n",
    "    <ul>\n",
    "       <li><a href=\"#6.1 Model B1 (Dropout)\">6.1 Model B1 (Dropout)</a></li>\n",
    "       <li><a href=\"#6.2 Model B2 (L2)\">6.2 Model B2 (L2)</a></li> \n",
    "       <li><a href=\"#6.3 Model B3 (Dropout+L2)\">6.3 Model B3 (Dropout+L2)</a></li>\n",
    "       <li><a href=\"#6.4 Fit\">6.4 Fit</a></li> \n",
    "       <li><a href=\"#6.5 Plot and Analysis\">6.5 Plot and Analysis</a></li> \n",
    "    </ul>\n",
    "    <li><a href=\"#7.Task 4: NN for regression task\">Task 4: NN for regression task</a></li>\n",
    "    <li><a href=\"#8.Task 5: NN for multi-classification task\">Task 5: NN for multi-classification task</a></li>\n",
    "    <li><a href=\"#9.Hyperparameter Explain\">Hyperparameter Explain</a></li>\n",
    "    <li><a href=\"#10.Unused Code\">Unused Code</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.Import Packages\"></a>\n",
    "# 3.Import Packages\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most of data we will store in \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# use regex to extract text\n",
    "import re\n",
    "# using copy to duplicate\n",
    "import copy\n",
    "# recording each step runing time\n",
    "import time\n",
    "# corpus will be string format\n",
    "import string\n",
    "# the easiest way to get text is using bs4 to get only text\n",
    "from bs4 import BeautifulSoup\n",
    "# make picture\n",
    "import matplotlib.pyplot as plt\n",
    "# draw picture\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# at EDA get Tokenizer info for deciding hyperparameters\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# add padding to a tokenized sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.Preprocess\"></a>\n",
    "# 4.Preprocess\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of 30 target labels are the same as the column names in the sample_submission.csv file. Target labels with the prefix <code>question_</code> relate to the <code>question_title</code> and/or <code>question_body</code> features in the data. Target labels with the prefix <code>answer_</code> relate to the <code>answer</code> feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.1 Import Data\"></a>\n",
    "### 4.1 Import Data\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess_data(object):\n",
    "    \"\"\"\n",
    "    Because this data have two components, so we first preprocess it and then return raw and corpus to do EDA\n",
    "    We also need to use preprocee to estimate hyperparameters\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        None\n",
    "    \n",
    "    def import_data(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        # read the raw unpreprocess data into df_raw\n",
    "        df_raw = pd.read_csv(\"03_data/02_train.csv\")\n",
    "        # first we need extract the X(data) part and y(label) part.\n",
    "        # In this dataset, columns from \"qa_id\" to \"host\" will be X(data)\n",
    "        # columns from \"question_asker_intent_understanding\" to \"answer_will_written\" are human label result which is numerical betwenn [0,1]\n",
    "        # \n",
    "        X_df = df_raw.iloc[:,0:10]\n",
    "        y_df = df_raw.iloc[:,11:]\n",
    "        \n",
    "        # we classify question_title and question_body in X_question_df, question_ related columns into y_question_df, this is X y for one task\n",
    "        # we classify answer in X_answer, answer_ related column into y_answer_df. This is X and y for another task\n",
    "        # maybe we need sometime consider questoin and answer together\n",
    "        # i believe user info have no contribution with this task\n",
    "        # construct DataFrame \n",
    "        self.X_question_df = df_raw.loc[:,['qa_id', 'question_title', 'question_body','category','host']]\n",
    "        self.X_answer_df = df_raw.loc[:,['qa_id','answer','category','host']]\n",
    "        \n",
    "        # initial label list\n",
    "        y_question_list = []\n",
    "        y_answer_list = []\n",
    "        for idx, i in enumerate(y_df.columns):\n",
    "            if \"question_\" in i:\n",
    "                y_question_list.append(i)\n",
    "            elif \"answer_\" in i:\n",
    "                y_answer_list.append(i)\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        # extract label of question\n",
    "        self.y_question_df = df_raw[y_question_list]\n",
    "        self.y_answer_df = df_raw[y_answer_list]\n",
    "        # So, for now, we have two pair of dataset, firt is (X_question_df + y_question_df). Second is (X_answer_df + y_answer_df)\n",
    "        \n",
    "        #for question part, i think we need a new column for merge title and body, but we still reserve seperate column\n",
    "        self.X_question_df['question'] = self.X_question_df['question_title'] + self.X_question_df['question_body']\n",
    "        \n",
    "        return df_raw, self.X_question_df, self.X_answer_df, self.y_question_df, self.y_answer_df\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.2 Clean Data\"></a>\n",
    "### 4.2 Clean Data\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class clean_data(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        None\n",
    "\n",
    "    def clean_process(self, df, column_1, punctuation_fun = True, stopwords_fun = False):\n",
    "        \"\"\"\n",
    "        I have reserve several interface and options to response different scenarios\n",
    "        \n",
    "        steps\n",
    "        1.lowercase()\n",
    "        2.remove special puncuation and symbol\n",
    "        4.remove string puncuation\n",
    "        5.only reserve words\n",
    "        6.remove stop words(if we eliminate most frequcen word, do we still this? and did this affect LSTM?) so i put a switch\n",
    "\n",
    "        Argus:\n",
    "        -----\n",
    "        df:DataFrame\n",
    "            Input come from import_data() [column_1] is the columns that need corpus,\n",
    "        column_1:string\n",
    "            the name of column we need clean\n",
    "        stopwrods_fun:boolen\n",
    "            True, remove stopwords; False, do not remove stopwords\n",
    "\n",
    "\n",
    "        Return:\n",
    "        ------\n",
    "        courpus_df:DataFrame\n",
    "            Because we use apply(lambda) to process each cell in DataFrame, so output is still that DataFrame\n",
    "        \"\"\"\n",
    "        print(\"*\"*50,\"Start Clean data\", \"*\"*50)\n",
    "        start_time = time.time()\n",
    "        #original datatype is serise, first transfrom to string and get lower() case text\n",
    "        corpus = df[column_1].str.lower()\n",
    "\n",
    "        # Because we don't have html characters,we don't need this part\n",
    "        # using bs4 to eliminate html\n",
    "        #soup = BeautifulSoup(corpus_2, 'lxml')\n",
    "        #corpus_3 = soup.get_text()\n",
    "        corpus = corpus.apply(lambda x: self.remove_html(x))\n",
    "        # any speical punctuation in filter sring should add \"\\\" before it\n",
    "        # This line is a compensation for remove_pun, if we puncturation_fun==False, which means we want to reserve some meanningfol\n",
    "        # symbols, we can use this seperate function to remove those meaningless symbols\n",
    "        corpus = corpus.apply(lambda x: self.remove_regex(x))\n",
    "\n",
    "        # if punctuation_fun == True, we remove punctuation. \n",
    "        #In this step, further we maybe need seperate them to keep question mark for represent some meaning\n",
    "        if punctuation_fun:\n",
    "            # use sring.puncutation to eliminate, but we should first remove url. I believe this should be last step\n",
    "            corpus = corpus.apply(lambda x: self.remove_pun(x))\n",
    "\n",
    "        # this is last safeguard for clean_data process. This function literally only extract \n",
    "        # only reserve words\n",
    "        pattern=r'[a-zA-Z][-._a-zA-Z]*[a-zA-Z]'\n",
    "        corpus = [\" \".join(re.findall(pattern, x)) for x in corpus]\n",
    "\n",
    "\n",
    "        if stopwords_fun:\n",
    "            # remove stop words\n",
    "            corpus_list = []\n",
    "            for sentence in tqdm(corpus_6):\n",
    "                corpus_list.append(self.remove_stopwords(sentence))\n",
    "    #         corpus_7 = corpus_6.apply(lambda x: remove_stopwords(x))\n",
    "    #         corpus_7 = [remove_stopwords(x) for x in corpus_6]\n",
    "            corpus = np.array(corpus_list)\n",
    "            \n",
    "#         # ********************improve strategy***************************** \n",
    "#         # due to this is very small, perhaps add stem and lemmanization will be better\n",
    "#         # According to Pro Liu, this step is not necessary\n",
    "#         # ********************improve strategy***************************** \n",
    "        \n",
    "        \n",
    "        #we sitll need to manipulate with index because it contain important relationship between corpus and claasify\n",
    "        #list to pd.Series\n",
    "#         corpus_8 = pd.Series(np.array(corpus))\n",
    "        df['cleaned'] = pd.Series(np.array(corpus))\n",
    "\n",
    "        cost_time = round((time.time()- start_time),4)\n",
    "        print(\"*\"*40,\"End clean_data() with {} second\".format(cost_time), \"*\"*40, end='\\n\\n')\n",
    "        \n",
    "\n",
    "\n",
    "        return df\n",
    "\n",
    "    def remove_pun(self, text):\n",
    "        \"\"\"\n",
    "        The reason i use seperate function is that pandas.apply can manipulate with column value. \n",
    "        If we use loop directly, we will get a join corpus without paragraph structure\n",
    "        \"\"\"\n",
    "        no_pun = \"\".join([c for c in text if c not in string.punctuation])\n",
    "        return no_pun\n",
    "\n",
    "    def remove_html(self, text):\n",
    "        \"\"\"\n",
    "        Use bs4 to extract text\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(text, 'lxml')\n",
    "        no_html = soup.get_text()\n",
    "        return no_html\n",
    "\n",
    "    def remove_regex(self, text):\n",
    "        \"\"\"\n",
    "        It was designed to remove special punctuation and character which we can't manipulate with string.puncutation effectively\n",
    "        #             #!\"#$%&()*\\+,-./:;<=>\\?@\\[\\\\\\]^_`{|}~\\\\t\\\\n\\\\r\\“\n",
    "        \"\"\"\n",
    "        #remove url\n",
    "        no_reg = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "        #remove numbers\n",
    "        no_reg = re.sub('\\w*\\d\\w*', '', no_reg)\n",
    "        return no_reg\n",
    "\n",
    "    def remove_stopwords(self, sentence):\n",
    "        stop_words_set = set(stopwords.words(\"english\"))\n",
    "        no_stopwords = \" \".join([word for word in sentence.split(\" \") if not word in stop_words_set])\n",
    "    #     for word in text:\n",
    "    #         if word not in stopwords.words(\"english\"):\n",
    "    #             output = output + \" \" + word\n",
    "        return no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.EDA\"></a>\n",
    "# 5.EDA\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eda_data(object):\n",
    "    \"\"\"\n",
    "    explorer this data structure\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.MAX_LENGTH = 100\n",
    "        \n",
    "        \n",
    "    def question_plot(self, df):\n",
    "        \"\"\"\n",
    "        Due to different column number, we need to \n",
    "        \"\"\"\n",
    "        #\n",
    "        fig, axes = plt.subplots(7, 3, figsize=(18, 15))\n",
    "        axes = axes.ravel()\n",
    "        bins = np.linspace(0, 1, 20)\n",
    "\n",
    "        for i, col in enumerate(df.columns):\n",
    "            ax = axes[i]\n",
    "            sns.histplot(df[col], label=col, kde=False, bins=bins, ax=ax)\n",
    "            # ax.set_title(col)\n",
    "            ax.set_xlim([0, 1])\n",
    "            ax.set_ylim([0, 6079])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "        \n",
    "    def tokenize_plot(self, question_cleaned_df, answer_cleaned_df):\n",
    "        \"\"\"\n",
    "        decide how many words should be left for tokenzie() function\n",
    "        \"\"\"\n",
    "        # do not set num_words at first time and to see how many unique words we have \n",
    "        tokenizer_model = Tokenizer(split=' ', char_level=False, oov_token=\"<OOV>\")\n",
    "        # here is another trick, we need consider question and answer courpus into one unit\n",
    "        # so we build a new big combination corpus\n",
    "        corpus_sum = question_cleaned_df['cleaned'] + answer_cleaned_df['cleaned']\n",
    "        # use previous model to fit this large combination corpus\n",
    "        tokenizer_model.fit_on_texts(corpus_sum)\n",
    "        # get the word_index and word_count dictionary\n",
    "        # word_index is the number corresponding to words by frequence. word_count is the sepcific words appeart times frequency\n",
    "        word_index, word_count = tokenizer_model.word_index, tokenizer_model.word_counts\n",
    "        print(f\"we got unique {len(word_index)} words\")\n",
    "        MAX_WORD=0\n",
    "        for i in word_count.values():\n",
    "            if i>5:\n",
    "                MAX_WORD+=1\n",
    "        print(f\"we have {MAX_WORD} words appear more than 5 times\")\n",
    "        \n",
    "                               \n",
    "        #**************new tokenize process******************\n",
    "        # start a new standard processs from begining\n",
    "        tokenizer_new = Tokenizer(num_words = MAX_WORD, split=' ', char_level=False, oov_token=\"<OOV>\")\n",
    "        # fit on combination corpus\n",
    "        tokenizer_new.fit_on_texts(corpus_sum)\n",
    "        # get question of sequence\n",
    "        question_seq = tokenizer_new.texts_to_sequences(question_cleaned_df['cleaned'])\n",
    "        # get padded\n",
    "        question_padded = pad_sequences(question_seq, padding='post', maxlen=self.MAX_LENGTH, truncating='post')\n",
    "        #**************new tokenize process******************\n",
    "        \n",
    "        return question_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we got unique 72070 words\n",
      "we have 13332 words appear more than 5 times\n"
     ]
    }
   ],
   "source": [
    "# eda_class = eda_data()\n",
    "# # eda_class.question_plot(y_question_df)\n",
    "# question_padded = eda_class.tokenize_plot(question_cleaned_df, answer_cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6079, 100)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6.Multi-Tokenize\"></a>\n",
    "# 6.Multi-Tokenize\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are seveal ways we can let model to learn how to provide score in each judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.TFIDF\n",
    "2.Embedding\n",
    "3.GLOVE\n",
    "4.LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF not word2vect, it just cacualte a numerical value for each word and concatenate into sentence to build a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(df, vectorizer=None, to_cat= True):\n",
    "    \"\"\"\n",
    "    Becuase we have to mentain \n",
    "    the relationship between corpus and his lable, we have several times list to DataFrame processes.\n",
    "    I didn't show EDA part for setting up \n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    df:pandas.DataFrame\n",
    "        DataFrame, contain ['AUTHOR'] and ['CLASSIFY']. This is raw data.\n",
    "        \n",
    "    vectorizer:cofig\n",
    "        When we apply to \n",
    "            \n",
    "    Return:\n",
    "    -------\n",
    "    X_vector:array\n",
    "        Dimension = (cleaned example, TOP_WORDS).Cleaned data and transformed to TFIDF format \n",
    "        with original sequence, which means can be matched withcooresponding y_labels.\n",
    "        \n",
    "    y_vector:\n",
    "        Dimension = (no. cleaned examples, NUM_LABELS)=(9630,3)\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"*\"*50,\"Start TFIDF transfrom\", \"*\"*50)\n",
    "    start_time = time.time()\n",
    "    # build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "    TOP_WORDS = 10000\n",
    "    \n",
    "    #tokens = corpus.apply(lambda x: x.split())\n",
    "    #tokens = word_tokenize(tokens)\n",
    "    #tokens = word_tokenize(corpus.str)\n",
    "    \n",
    "    #Stemming and Lemmatizing:   \n",
    "    #Coreference resolution\n",
    "    \n",
    "   \n",
    "     #*****************************cut down Part of optimization******************************************\n",
    "#     #we can't directly drop column because index will dynamicly decrease, but we can gather index of drop\n",
    "#     index_drop=[]\n",
    "#     #iteration through all DataFrame\n",
    "#     for i in range(len(df)):\n",
    "#         #according to plot and statitc result,>600 have 2628, <60 have 3540, so we only need 60<data<600\n",
    "#         if len(df.iloc[i,0])<60 or len(df.iloc[i,0])>600:\n",
    "#             #delete too big and too small\n",
    "#             index_drop.append(i)\n",
    "#     print(f\"before drop shape={df.shape}\")\n",
    "    \n",
    "#     #drop row by list but remain old index number\n",
    "#     df = df.drop(index_drop,axis=0)\n",
    "#     print(f\"after drop shape={df.shape}\")\n",
    "#     #re.finall(\\w+)  \n",
    "#     #*****************************cut down Part of optimization******************************************\n",
    "    \n",
    "    # inintial we need use vectorizer from train to process test data\n",
    "    if vectorizer == None:\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', max_features = TOP_WORDS)\n",
    "        #when we built a vect, we need to import all words as corpus combination to generate a TFIDF dictornary\n",
    "        vect = vectorizer.fit([\" \".join(df['AUTHOR'].tolist())])\n",
    "        #after fit, we need to use dictionary to transform our coupus into TFIDF(we can use sum() to check sparse)\n",
    "        data_2 = vect.transform(df['AUTHOR']).toarray()\n",
    "        X_vector = np.array(data_2)\n",
    "    else:\n",
    "        vect = vectorizer.fit([\" \".join(df['AUTHOR'].tolist())])\n",
    "        #after fit, we need to use dictionary to transform our coupus into TFIDF(we can use sum() to check sparse)\n",
    "        X_vector = vect.transform(df['AUTHOR']).toarray()\n",
    "\n",
    "    if to_categorical == True:\n",
    "        #To remain the relation between AUTHOR vector and CLASSIFY labels, we also output y_vector coorespondingly\n",
    "        y_vector = to_categorical(np.array(df['CLASSIFY']))\n",
    "    else:\n",
    "        y_vector = df['CLASSIFY']\n",
    "        \n",
    "    cost_time = round((time.time()- start_time),4)\n",
    "    print(\"*\"*40,\"End tfidf() with {} second\".format(cost_time), \"*\"*40, end='\\n\\n')\n",
    "    \n",
    "    return df, X_vector, y_vector, vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot\n",
    "BLUE Rougue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7.Classify\"></a>\n",
    "# 7.Classify\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will provide more than three type of input. All matrix will use number represent a whole sentence. What we can do include surpivsed learning to classify (NN, KNN) and unsurprivised learning to cluster (k-means + PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7.1 KNN\"></a>\n",
    "### 7.1 KNN\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7.2 Neural Network Classify\"></a>\n",
    "### 7.2 Neural Network Classify\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tranform category to one-hot, last layer use softmax, use argmax() to get predict result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7.3 K-means cluster + PCA\"></a>\n",
    "### 7.3 K-means cluster + PCA\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get high dimentsion cluster result and reduce dimenstion to get a 3D/2D visulization result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8.Models\"></a>\n",
    "# 8.Models\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.CNN(u-gim, bi, trigum = contcant)\n",
    "2.CNN \n",
    "3.GRU\n",
    "4.BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9.Main Function\"></a>\n",
    "# 9.Main Function\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_function():\n",
    "    \"\"\"\n",
    "    We use this function to call process one by one.\n",
    "    \"\"\"\n",
    "    pre = preprocess_data()\n",
    "    df_raw, X_question_df, X_answer_df, y_question_df, y_answer_df = pre.import_data()\n",
    "\n",
    "    clean = clean_data()\n",
    "    question_cleaned_df = clean.clean_process(X_question_df, column_1 ='question')\n",
    "    answer_cleaned_df = clean.clean_process(X_answer_df, column_1='answer')\n",
    "    \n",
    "    eda_class = eda_data()\n",
    "    # eda_class.question_plot(y_question_df)\n",
    "    # question_padded have shape (6079,100) can be used in fewer embedding\n",
    "    question_padded = eda_class.tokenize_plot(question_cleaned_df, answer_cleaned_df)\n",
    "    \n",
    "    return df_raw, X_question_df, X_answer_df, y_question_df, y_answer_df, question_cleaned_df, answer_cleaned_df, question_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** Start Clean data **************************************************\n",
      "**************************************** End clean_data() with 3.4431 second ****************************************\n",
      "\n",
      "************************************************** Start Clean data **************************************************\n",
      "**************************************** End clean_data() with 3.2384 second ****************************************\n",
      "\n",
      "we got unique 72070 words\n",
      "we have 13332 words appear more than 5 times\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    df_raw, X_question_df, X_answer_df, y_question_df, y_answer_df, question_cleaned_df, answer_cleaned_df, question_padded = main_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>category</th>\n",
       "      <th>host</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I just got extension tubes, so here's the skin...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>photo.stackexchange.com</td>\n",
       "      <td>just got extension tubes so heres the skinny w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It might be helpful to look into the definitio...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>rpg.stackexchange.com</td>\n",
       "      <td>it might be helpful to look into the definitio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Do you even need grooves?  We make several pro...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>electronics.stackexchange.com</td>\n",
       "      <td>do you even need grooves we make several produ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sending an \"affidavit\" it is a dispute between...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>judaism.stackexchange.com</td>\n",
       "      <td>sending an affidavit it is dispute between ras...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Check out Image Trace in Adobe Illustrator. \\n...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>graphicdesign.stackexchange.com</td>\n",
       "      <td>check out image trace in adobe illustrator lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6074</th>\n",
       "      <td>9642</td>\n",
       "      <td>If you're thinking about wearing a ski helmet ...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>bicycles.stackexchange.com</td>\n",
       "      <td>if youre thinking about wearing ski helmet for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6075</th>\n",
       "      <td>9643</td>\n",
       "      <td>\\nYou can replace the pads (as stated elsewher...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>bicycles.stackexchange.com</td>\n",
       "      <td>you can replace the pads as stated elsewhere t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6076</th>\n",
       "      <td>9645</td>\n",
       "      <td>Maybe help if can be fixes origin of this erro...</td>\n",
       "      <td>TECHNOLOGY</td>\n",
       "      <td>unix.stackexchange.com</td>\n",
       "      <td>maybe help if can be fixes origin of this erro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6077</th>\n",
       "      <td>9646</td>\n",
       "      <td>As a non-mathematician, I am somewhat mystifie...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>mathoverflow.net</td>\n",
       "      <td>as nonmathematician am somewhat mystified by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6078</th>\n",
       "      <td>9647</td>\n",
       "      <td>First, I really like Eric's answer for practic...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>diy.stackexchange.com</td>\n",
       "      <td>first really like erics answer for practical r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6079 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      qa_id                                             answer    category  \\\n",
       "0         0  I just got extension tubes, so here's the skin...   LIFE_ARTS   \n",
       "1         1  It might be helpful to look into the definitio...     CULTURE   \n",
       "2         2  Do you even need grooves?  We make several pro...     SCIENCE   \n",
       "3         3  Sending an \"affidavit\" it is a dispute between...     CULTURE   \n",
       "4         5  Check out Image Trace in Adobe Illustrator. \\n...   LIFE_ARTS   \n",
       "...     ...                                                ...         ...   \n",
       "6074   9642  If you're thinking about wearing a ski helmet ...     CULTURE   \n",
       "6075   9643  \\nYou can replace the pads (as stated elsewher...     CULTURE   \n",
       "6076   9645  Maybe help if can be fixes origin of this erro...  TECHNOLOGY   \n",
       "6077   9646  As a non-mathematician, I am somewhat mystifie...     SCIENCE   \n",
       "6078   9647  First, I really like Eric's answer for practic...   LIFE_ARTS   \n",
       "\n",
       "                                 host  \\\n",
       "0             photo.stackexchange.com   \n",
       "1               rpg.stackexchange.com   \n",
       "2       electronics.stackexchange.com   \n",
       "3           judaism.stackexchange.com   \n",
       "4     graphicdesign.stackexchange.com   \n",
       "...                               ...   \n",
       "6074       bicycles.stackexchange.com   \n",
       "6075       bicycles.stackexchange.com   \n",
       "6076           unix.stackexchange.com   \n",
       "6077                 mathoverflow.net   \n",
       "6078            diy.stackexchange.com   \n",
       "\n",
       "                                                cleaned  \n",
       "0     just got extension tubes so heres the skinny w...  \n",
       "1     it might be helpful to look into the definitio...  \n",
       "2     do you even need grooves we make several produ...  \n",
       "3     sending an affidavit it is dispute between ras...  \n",
       "4     check out image trace in adobe illustrator lik...  \n",
       "...                                                 ...  \n",
       "6074  if youre thinking about wearing ski helmet for...  \n",
       "6075  you can replace the pads as stated elsewhere t...  \n",
       "6076  maybe help if can be fixes origin of this erro...  \n",
       "6077  as nonmathematician am somewhat mystified by t...  \n",
       "6078  first really like erics answer for practical r...  \n",
       "\n",
       "[6079 rows x 5 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_title</th>\n",
       "      <th>question_body</th>\n",
       "      <th>category</th>\n",
       "      <th>host</th>\n",
       "      <th>question</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What am I losing when using extension tubes in...</td>\n",
       "      <td>After playing around with macro photography on...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>photo.stackexchange.com</td>\n",
       "      <td>What am I losing when using extension tubes in...</td>\n",
       "      <td>what am losing when using extension tubes inst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the distinction between a city and a s...</td>\n",
       "      <td>I am trying to understand what kinds of places...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>rpg.stackexchange.com</td>\n",
       "      <td>What is the distinction between a city and a s...</td>\n",
       "      <td>what is the distinction between city and spraw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Maximum protusion length for through-hole comp...</td>\n",
       "      <td>I'm working on a PCB that has through-hole com...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>electronics.stackexchange.com</td>\n",
       "      <td>Maximum protusion length for through-hole comp...</td>\n",
       "      <td>maximum protusion length for throughhole compo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id                                     question_title  \\\n",
       "0      0  What am I losing when using extension tubes in...   \n",
       "1      1  What is the distinction between a city and a s...   \n",
       "2      2  Maximum protusion length for through-hole comp...   \n",
       "\n",
       "                                       question_body   category  \\\n",
       "0  After playing around with macro photography on...  LIFE_ARTS   \n",
       "1  I am trying to understand what kinds of places...    CULTURE   \n",
       "2  I'm working on a PCB that has through-hole com...    SCIENCE   \n",
       "\n",
       "                            host  \\\n",
       "0        photo.stackexchange.com   \n",
       "1          rpg.stackexchange.com   \n",
       "2  electronics.stackexchange.com   \n",
       "\n",
       "                                            question  \\\n",
       "0  What am I losing when using extension tubes in...   \n",
       "1  What is the distinction between a city and a s...   \n",
       "2  Maximum protusion length for through-hole comp...   \n",
       "\n",
       "                                             cleaned  \n",
       "0  what am losing when using extension tubes inst...  \n",
       "1  what is the distinction between city and spraw...  \n",
       "2  maximum protusion length for throughhole compo...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_question_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>category</th>\n",
       "      <th>host</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I just got extension tubes, so here's the skin...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>photo.stackexchange.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It might be helpful to look into the definitio...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>rpg.stackexchange.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Do you even need grooves?  We make several pro...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>electronics.stackexchange.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sending an \"affidavit\" it is a dispute between...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>judaism.stackexchange.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Check out Image Trace in Adobe Illustrator. \\n...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>graphicdesign.stackexchange.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id                                             answer   category  \\\n",
       "0      0  I just got extension tubes, so here's the skin...  LIFE_ARTS   \n",
       "1      1  It might be helpful to look into the definitio...    CULTURE   \n",
       "2      2  Do you even need grooves?  We make several pro...    SCIENCE   \n",
       "3      3  Sending an \"affidavit\" it is a dispute between...    CULTURE   \n",
       "4      5  Check out Image Trace in Adobe Illustrator. \\n...  LIFE_ARTS   \n",
       "\n",
       "                              host  \n",
       "0          photo.stackexchange.com  \n",
       "1            rpg.stackexchange.com  \n",
       "2    electronics.stackexchange.com  \n",
       "3        judaism.stackexchange.com  \n",
       "4  graphicdesign.stackexchange.com  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_answer_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>question_not_really_a_question</th>\n",
       "      <th>...</th>\n",
       "      <th>question_type_choice</th>\n",
       "      <th>question_type_compare</th>\n",
       "      <th>question_type_consequence</th>\n",
       "      <th>question_type_definition</th>\n",
       "      <th>question_type_entity</th>\n",
       "      <th>question_type_instructions</th>\n",
       "      <th>question_type_procedure</th>\n",
       "      <th>question_type_reason_explanation</th>\n",
       "      <th>question_type_spelling</th>\n",
       "      <th>question_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_asker_intent_understanding  question_body_critical  \\\n",
       "0                             1.000000                0.333333   \n",
       "1                             1.000000                1.000000   \n",
       "2                             0.888889                0.666667   \n",
       "3                             0.888889                0.666667   \n",
       "4                             1.000000                0.666667   \n",
       "\n",
       "   question_conversational  question_expect_short_answer  \\\n",
       "0                 0.000000                           0.0   \n",
       "1                 0.000000                           0.5   \n",
       "2                 0.000000                           1.0   \n",
       "3                 0.666667                           1.0   \n",
       "4                 0.000000                           1.0   \n",
       "\n",
       "   question_fact_seeking  question_has_commonly_accepted_answer  \\\n",
       "0                    0.0                                    0.0   \n",
       "1                    1.0                                    1.0   \n",
       "2                    1.0                                    1.0   \n",
       "3                    1.0                                    1.0   \n",
       "4                    1.0                                    1.0   \n",
       "\n",
       "   question_interestingness_others  question_interestingness_self  \\\n",
       "0                         1.000000                       1.000000   \n",
       "1                         0.444444                       0.444444   \n",
       "2                         0.666667                       0.444444   \n",
       "3                         0.444444                       0.444444   \n",
       "4                         0.666667                       0.666667   \n",
       "\n",
       "   question_multi_intent  question_not_really_a_question  ...  \\\n",
       "0               0.000000                             0.0  ...   \n",
       "1               0.666667                             0.0  ...   \n",
       "2               0.333333                             0.0  ...   \n",
       "3               0.000000                             0.0  ...   \n",
       "4               0.000000                             0.0  ...   \n",
       "\n",
       "   question_type_choice  question_type_compare  question_type_consequence  \\\n",
       "0              0.000000               0.000000                        0.0   \n",
       "1              0.666667               0.666667                        0.0   \n",
       "2              0.000000               0.000000                        0.0   \n",
       "3              1.000000               0.000000                        0.0   \n",
       "4              0.000000               0.000000                        0.0   \n",
       "\n",
       "   question_type_definition  question_type_entity  question_type_instructions  \\\n",
       "0                  0.000000                   0.0                         1.0   \n",
       "1                  0.333333                   0.0                         0.0   \n",
       "2                  0.000000                   0.0                         1.0   \n",
       "3                  0.000000                   0.0                         0.0   \n",
       "4                  0.000000                   0.0                         1.0   \n",
       "\n",
       "   question_type_procedure  question_type_reason_explanation  \\\n",
       "0                 0.000000                          0.000000   \n",
       "1                 0.000000                          0.333333   \n",
       "2                 0.333333                          0.333333   \n",
       "3                 0.000000                          0.000000   \n",
       "4                 0.000000                          1.000000   \n",
       "\n",
       "   question_type_spelling  question_well_written  \n",
       "0                     0.0               1.000000  \n",
       "1                     0.0               0.888889  \n",
       "2                     0.0               0.777778  \n",
       "3                     0.0               0.888889  \n",
       "4                     0.0               1.000000  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_question_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"10.Unused Code\"></a>\n",
    "# 10.Unused Code\n",
    "<a href=\"#1.Summary\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.using question_user_page as benchmark, split quetsion_title or question_body into train_test, we believe different website have different type questions, so we can make evaluate and predict model\n",
    "\n",
    "2.Using unsupervise learning to cluster question into differnt type, culster is depending on data preprocessing granularity. smaller grandularity, more spase cluster\n",
    "\n",
    "3.After i embedding these sentence, you can use KNN SVM to do unsuperviese cluster\n",
    "\n",
    "4.Using categore to cluster by CNN(n-gram / Glove / miniGPT）\n",
    "\n",
    "5.Generage numerical value by former data and compart to anser_well_written\n",
    "\n",
    "6.extract the root url like photo.stackchange.com to try to classfiy it with some argothrim, same question is to catgory column\n",
    "\n",
    "7.If the result is not good enough, try to use url to grab more data to analysis\n",
    "\n",
    "8.The data for this competition includes questions and answers from various StackExchange properties. Your task is to predict target values of 30 labels for each question-answer pair.\n",
    "\n",
    "The list of 30 target labels are the same as the column names in the sample_submission.csv file. Target labels with the prefix question_ relate to the question_title and/or question_body features in the data. Target labels with the prefix answer_ relate to the answer feature.\n",
    "\n",
    "9.for each dataframe maybe we need add category, and that will imporove performance\n",
    "\n",
    "10.Stopword is meaningful for answer sequence, and so as punctuation. Try to only eliminate useless punctuatinon like '\\`' but remain '?'and '!'\n",
    "11.embedding is random initial word vector, but we can use Glove to import pretrain to impove performance\n",
    "\n",
    "12.evalution part try to use BLEU score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.tensorflow.org/tutorials/text/word_embeddings?hl=zh-cn\n",
    "- https://towardsdatascience.com/nlp-building-a-question-answering-model-ed0529a68c54"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
