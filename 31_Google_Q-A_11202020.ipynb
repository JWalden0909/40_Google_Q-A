{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.Summary\"></a>\n",
    "# 1.Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.kaggle.com/c/google-quest-challenge\n",
    "- BERT https://zhuanlan.zhihu.com/p/269516862"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1.EDA\n",
    "- 2.Preprocess data\n",
    "- 3.Classify\n",
    "- 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, we have a Q&A dataset and there are 30+ columns `[0,1]` data coming from human evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.Table of Contents\"></a>\n",
    "# 2.Tabole of Contents\n",
    "<a href=\"#1.Summary\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li><a href=\"#1.Summary\">Summary</a></li>\n",
    "    <li><a href=\"#2.Tabole of Contents\">Tabole of Contents</a></li>\n",
    "    <li><a href=\"#3.Import Packages\">Import Packages</a></li>\n",
    "    <li><a href=\"#4.Preprocess\">Preprocess</a></li>\n",
    "    <ul>\n",
    "       <li><a href=\"#4.1 Import Data\">4.1 Import Data</a></li>\n",
    "       <li><a href=\"#4.2 Clean Data\">4.2 Clean Data</a></li> \n",
    "    </ul>\n",
    "    <li><a href=\"#5.EDA\">EDA</a></li>\n",
    "    <ul>\n",
    "       <li><a href=\"#5.1 Model_A\">5.1 Model_A</a></li>\n",
    "       <li><a href=\"#5.2 Model_B\">5.2 Model_B</a></li> \n",
    "       <li><a href=\"#5.3 Model_C\">5.3 Model_C</a></li>\n",
    "       <li><a href=\"#5.4 Split and Fit\">5.4 Split and Fit</a></li>\n",
    "       <li><a href=\"#5.5 Main_Function\">5.5 Main_Function</a></li> \n",
    "       <li><a href=\"#5.6 Plot and Analysis\">5.6 Plot and Analysis</a></li> \n",
    "    </ul>\n",
    "    <li><a href=\"#6.Task 3: Fight overfitting\">Task 3: Fight overfitting</a></li>\n",
    "    <ul>\n",
    "       <li><a href=\"#6.1 Model B1 (Dropout)\">6.1 Model B1 (Dropout)</a></li>\n",
    "       <li><a href=\"#6.2 Model B2 (L2)\">6.2 Model B2 (L2)</a></li> \n",
    "       <li><a href=\"#6.3 Model B3 (Dropout+L2)\">6.3 Model B3 (Dropout+L2)</a></li>\n",
    "       <li><a href=\"#6.4 Fit\">6.4 Fit</a></li> \n",
    "       <li><a href=\"#6.5 Plot and Analysis\">6.5 Plot and Analysis</a></li> \n",
    "    </ul>\n",
    "    <li><a href=\"#7.Classify\">Classify</a></li>\n",
    "    <li><a href=\"#8.Task 5: NN for multi-classification task\">Task 5: NN for multi-classification task</a></li>\n",
    "    <li><a href=\"#9.Main Function\">Main Function</a></li>\n",
    "    <li><a href=\"#10.Unused Code\">Unused Code</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.Import Packages\"></a>\n",
    "# 3.Import Packages\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most of data we will store in \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# use regex to extract text\n",
    "import re\n",
    "# using copy to duplicate\n",
    "import copy\n",
    "# recording each step runing time\n",
    "import time\n",
    "# corpus will be string format\n",
    "import string\n",
    "# the easiest way to get text is using bs4 to get only text\n",
    "from bs4 import BeautifulSoup\n",
    "# make picture\n",
    "import matplotlib.pyplot as plt\n",
    "# draw picture\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# one of tokenize method\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# split data with random seed (37)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# at EDA get Tokenizer info for deciding hyperparameters\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# add padding to a tokenized sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# input all the layers we might use \n",
    "from tensorflow.keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, \\\n",
    "Dropout, Activation, Input, Flatten, Concatenate, LSTM, GlobalAveragePooling1D\n",
    "# do not use sequential to build model\n",
    "from tensorflow.keras import Model\n",
    "# need specify lr in optiizer\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.Preprocess\"></a>\n",
    "# 4.Preprocess\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of 30 target labels are the same as the column names in the sample_submission.csv file. Target labels with the prefix <code>question_</code> relate to the <code>question_title</code> and/or <code>question_body</code> features in the data. Target labels with the prefix <code>answer_</code> relate to the <code>answer</code> feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.1 Import Data\"></a>\n",
    "### 4.1 Import Data\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess_data(object):\n",
    "    \"\"\"\n",
    "    Because this data have two components, so we first preprocess it and then return raw and corpus to do EDA\n",
    "    We also need to use preprocee to estimate hyperparameters\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        None\n",
    "    \n",
    "    def import_data(self,path):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        # read the raw unpreprocess data into df_raw\n",
    "        df_raw = pd.read_csv(path)\n",
    "        # first we need extract the X(data) part and y(label) part.\n",
    "        # In this dataset, columns from \"qa_id\" to \"host\" will be X(data)\n",
    "        # columns from \"question_asker_intent_understanding\" to \"answer_will_written\" are human label result which is numerical betwenn [0,1]\n",
    "        # \n",
    "        X_df = df_raw.iloc[:,0:10]\n",
    "        y_df = df_raw.iloc[:,11:]\n",
    "        \n",
    "        # we classify question_title and question_body in X_question_df, question_ related columns into y_question_df, this is X y for one task\n",
    "        # we classify answer in X_answer, answer_ related column into y_answer_df. This is X and y for another task\n",
    "        # maybe we need sometime consider questoin and answer together\n",
    "        # i believe user info have no contribution with this task\n",
    "        # construct DataFrame \n",
    "        self.X_question_df = df_raw.loc[:,['qa_id', 'question_title', 'question_body','category','host']]\n",
    "        self.X_answer_df = df_raw.loc[:,['qa_id','answer','category','host']]\n",
    "        \n",
    "        # initial label list\n",
    "        y_question_list = []\n",
    "        y_answer_list = []\n",
    "        for idx, i in enumerate(y_df.columns):\n",
    "            if \"question_\" in i:\n",
    "                y_question_list.append(i)\n",
    "            elif \"answer_\" in i:\n",
    "                y_answer_list.append(i)\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        # extract label of question\n",
    "        self.y_question_df = df_raw[y_question_list]\n",
    "        self.y_answer_df = df_raw[y_answer_list]\n",
    "        # So, for now, we have two pair of dataset, firt is (X_question_df + y_question_df). Second is (X_answer_df + y_answer_df)\n",
    "        \n",
    "        #for question part, i think we need a new column for merge title and body, but we still reserve seperate column\n",
    "        self.X_question_df['question'] = self.X_question_df['question_title'] + self.X_question_df['question_body']\n",
    "        \n",
    "        return df_raw, self.X_question_df, self.X_answer_df, self.y_question_df, self.y_answer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.2 Clean Data\"></a>\n",
    "### 4.2 Clean Data\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class clean_data(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        None\n",
    "\n",
    "    def clean_process(self, df, column_1, punctuation_fun = True, stopwords_fun = False):\n",
    "        \"\"\"\n",
    "        I have reserve several interface and options to response different scenarios\n",
    "        \n",
    "        steps\n",
    "        1.lowercase()\n",
    "        2.remove special puncuation and symbol\n",
    "        4.remove string puncuation\n",
    "        5.only reserve words\n",
    "        6.remove stop words(if we eliminate most frequcen word, do we still this? and did this affect LSTM?) so i put a switch\n",
    "\n",
    "        Argus:\n",
    "        -----\n",
    "        df:DataFrame\n",
    "            Input come from import_data() [column_1] is the columns that need corpus,\n",
    "        column_1:string\n",
    "            the name of column we need clean\n",
    "        stopwrods_fun:boolen\n",
    "            True, remove stopwords; False, do not remove stopwords\n",
    "\n",
    "\n",
    "        Return:\n",
    "        ------\n",
    "        courpus_df:DataFrame\n",
    "            Because we use apply(lambda) to process each cell in DataFrame, so output is still that DataFrame\n",
    "        \"\"\"\n",
    "        print(\"*\"*50,\"Start Clean data\", \"*\"*50)\n",
    "        start_time = time.time()\n",
    "        #original datatype is serise, first transfrom to string and get lower() case text\n",
    "        corpus = df[column_1].str.lower()\n",
    "\n",
    "        # Because we don't have html characters,we don't need this part\n",
    "        # using bs4 to eliminate html\n",
    "        #soup = BeautifulSoup(corpus_2, 'lxml')\n",
    "        #corpus_3 = soup.get_text()\n",
    "        corpus = corpus.apply(lambda x: self.remove_html(x))\n",
    "        # any speical punctuation in filter sring should add \"\\\" before it\n",
    "        # This line is a compensation for remove_pun, if we puncturation_fun==False, which means we want to reserve some meanningfol\n",
    "        # symbols, we can use this seperate function to remove those meaningless symbols\n",
    "        corpus = corpus.apply(lambda x: self.remove_regex(x))\n",
    "\n",
    "        # if punctuation_fun == True, we remove punctuation. \n",
    "        #In this step, further we maybe need seperate them to keep question mark for represent some meaning\n",
    "        if punctuation_fun:\n",
    "            # use sring.puncutation to eliminate, but we should first remove url. I believe this should be last step\n",
    "            corpus = corpus.apply(lambda x: self.remove_pun(x))\n",
    "\n",
    "        # this is last safeguard for clean_data process. This function literally only extract \n",
    "        # only reserve words\n",
    "        pattern=r'[a-zA-Z][-._a-zA-Z]*[a-zA-Z]'\n",
    "        corpus = [\" \".join(re.findall(pattern, x)) for x in corpus]\n",
    "\n",
    "\n",
    "        if stopwords_fun:\n",
    "            # remove stop words\n",
    "            corpus_list = []\n",
    "            for sentence in tqdm(corpus_6):\n",
    "                corpus_list.append(self.remove_stopwords(sentence))\n",
    "    #         corpus_7 = corpus_6.apply(lambda x: remove_stopwords(x))\n",
    "    #         corpus_7 = [remove_stopwords(x) for x in corpus_6]\n",
    "            corpus = np.array(corpus_list)\n",
    "            \n",
    "#         # ********************improve strategy***************************** \n",
    "#         # due to this is very small, perhaps add stem and lemmanization will be better\n",
    "#         # According to Pro Liu, this step is not necessary\n",
    "#         # ********************improve strategy***************************** \n",
    "        \n",
    "        \n",
    "        #we sitll need to manipulate with index because it contain important relationship between corpus and claasify\n",
    "        #list to pd.Series\n",
    "#         corpus_8 = pd.Series(np.array(corpus))\n",
    "        df['cleaned'] = pd.Series(np.array(corpus))\n",
    "\n",
    "        cost_time = round((time.time()- start_time),4)\n",
    "        print(\"*\"*40,\"End clean_data() with {} second\".format(cost_time), \"*\"*40, end='\\n\\n')\n",
    "        \n",
    "\n",
    "\n",
    "        return df\n",
    "\n",
    "    def remove_pun(self, text):\n",
    "        \"\"\"\n",
    "        The reason i use seperate function is that pandas.apply can manipulate with column value. \n",
    "        If we use loop directly, we will get a join corpus without paragraph structure\n",
    "        \"\"\"\n",
    "        no_pun = \"\".join([c for c in text if c not in string.punctuation])\n",
    "        return no_pun\n",
    "\n",
    "    def remove_html(self, text):\n",
    "        \"\"\"\n",
    "        Use bs4 to extract text\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(text, 'lxml')\n",
    "        no_html = soup.get_text()\n",
    "        return no_html\n",
    "\n",
    "    def remove_regex(self, text):\n",
    "        \"\"\"\n",
    "        It was designed to remove special punctuation and character which we can't manipulate with string.puncutation effectively\n",
    "        #             #!\"#$%&()*\\+,-./:;<=>\\?@\\[\\\\\\]^_`{|}~\\\\t\\\\n\\\\r\\“\n",
    "        \"\"\"\n",
    "        #remove url\n",
    "        no_reg = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "        #remove numbers\n",
    "        no_reg = re.sub('\\w*\\d\\w*', '', no_reg)\n",
    "        return no_reg\n",
    "\n",
    "    def remove_stopwords(self, sentence):\n",
    "        stop_words_set = set(stopwords.words(\"english\"))\n",
    "        no_stopwords = \" \".join([word for word in sentence.split(\" \") if not word in stop_words_set])\n",
    "    #     for word in text:\n",
    "    #         if word not in stopwords.words(\"english\"):\n",
    "    #             output = output + \" \" + word\n",
    "        return no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.EDA\"></a>\n",
    "# 5.EDA\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eda_data(object):\n",
    "    \"\"\"\n",
    "    explorer this data structure\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # using distribution to decide this parameters\n",
    "        self.MAX_SEQ_LENGTH = 300\n",
    "        \n",
    "        \n",
    "    def question_plot(self, df):\n",
    "        \"\"\"\n",
    "        Due to different column number, we need to \n",
    "        \"\"\"\n",
    "        #\n",
    "        fig, axes = plt.subplots(7, 3, figsize=(18, 15))\n",
    "        axes = axes.ravel()\n",
    "        bins = np.linspace(0, 1, 20)\n",
    "\n",
    "        for i, col in enumerate(df.columns):\n",
    "            ax = axes[i]\n",
    "            sns.histplot(df[col], label=col, kde=False, bins=bins, ax=ax)\n",
    "            # ax.set_title(col)\n",
    "            ax.set_xlim([0, 1])\n",
    "            ax.set_ylim([0, 6079])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "    def tokenize_plot(self, question_cleaned_df, answer_cleaned_df):\n",
    "        \"\"\"\n",
    "        decide how many words should be left for tokenzie() function\n",
    "        \"\"\"\n",
    "        # do not set num_words at first time and to see how many unique words we have \n",
    "        tokenizer_model = Tokenizer(split=' ', char_level=False, oov_token=\"<OOV>\")\n",
    "        # here is another trick, we need consider question and answer courpus into one unit\n",
    "        # so we build a new big combination corpus\n",
    "        corpus_sum = question_cleaned_df['cleaned'] + answer_cleaned_df['cleaned']\n",
    "        # use previous model to fit this large combination corpus\n",
    "        tokenizer_model.fit_on_texts(corpus_sum)\n",
    "        # get the word_index and word_count dictionary\n",
    "        # word_index is the number corresponding to words by frequence. word_count is the sepcific words appeart times frequency\n",
    "        word_index, word_count = tokenizer_model.word_index, tokenizer_model.word_counts\n",
    "        print(f\"we got unique {len(word_index)} words\")\n",
    "        MAX_WORD=0\n",
    "        for i in word_count.values():\n",
    "            if i>5:\n",
    "                MAX_WORD+=1\n",
    "        print(f\"we have {MAX_WORD} words appear more than 5 times\")\n",
    "        \n",
    "                               \n",
    "        #**************new tokenize process******************\n",
    "        # start a new standard processs from begining\n",
    "        tokenizer_new = Tokenizer(num_words = MAX_WORD, split=' ', char_level=False, oov_token=\"<OOV>\")\n",
    "        # fit on combination corpus\n",
    "        tokenizer_new.fit_on_texts(corpus_sum)\n",
    "        # get question of sequence\n",
    "        question_seq = tokenizer_new.texts_to_sequences(question_cleaned_df['cleaned'])\n",
    "        # get padded\n",
    "        question_padded = pad_sequences(question_seq, padding='post', maxlen=self.MAX_SEQ_LENGTH, truncating='post')\n",
    "        #**************new tokenize process******************\n",
    "        question_cleaned_df['padded'] = list(question_padded)\n",
    "        \n",
    "        return question_padded, question_cleaned_df, word_index\n",
    "    \n",
    "    \n",
    "    def label_feature(self, y_question_df):\n",
    "        \"\"\"\n",
    "        In future, i will use arguritem to filter column. For now, i do it manually\n",
    "        As i test use pure numerical algorithem to calcualte \n",
    "        \"\"\"\n",
    "        # first try these labels\n",
    "        feature_cols = ['question_asker_intent_understanding',\n",
    "                       'question_body_critical',\n",
    "                       'question_expect_short_answer',\n",
    "                       'question_interestingness_others'\n",
    "                      ]\n",
    "        y_label_df = y_question_df[feature_cols]\n",
    "        return y_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_question_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a3dc5022b846>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0meda_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meda_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0meda_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquestion_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_question_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mquestion_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion_cleaned_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meda_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion_cleaned_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswer_cleaned_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_label_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meda_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_question_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_question_df' is not defined"
     ]
    }
   ],
   "source": [
    "# eda_class = eda_data()\n",
    "# eda_class.question_plot(y_question_df)\n",
    "# question_padded, question_cleaned_df, word_index = eda_class.tokenize_plot(question_cleaned_df, answer_cleaned_df)\n",
    "# y_label_df = eda_class.label_feature(y_question_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_question_df.loc[:,'labels'] = y_question_df.loc[:,feature_cols].apply(lambda x: ','.join(x.astype(str)),axis=1)\n",
    "\n",
    "# y_question_df['labels'] = y_question_df[feature_cols].apply(lambda x: ','.join(x.astype(str)),axis=1)\n",
    "# y_question_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6.Multi-Tokenize\"></a>\n",
    "# 6.Multi-Tokenize\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are seveal ways we can let model to learn how to provide score in each judgement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1.TFIDF ( if using max_df and min_df, perhaps not fit seq2seq generation model)\n",
    "- 2.Embedding\n",
    "- 3.GLOVE\n",
    "- 4.LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF not word2vect, it just cacualte a numerical value for each word and concatenate into sentence to build a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tfidf_data(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        initial hyperparameters\n",
    "        \"\"\"\n",
    "        # build a vocabulary that only consider the top max_features ordered by term frequency across the corpus. learned from EDA\n",
    "        self.MAX_WORD = 13332\n",
    "        \n",
    "    def tfidf(df, vectorizer=None, to_cat= True):\n",
    "        \"\"\"\n",
    "        Becuase we have to mentain \n",
    "        the relationship between corpus and his lable, we have several times list to DataFrame processes.\n",
    "        I didn't show EDA part for setting up \n",
    "\n",
    "        Parameters:\n",
    "        ------------\n",
    "        df:pandas.DataFrame\n",
    "            DataFrame, contain ['AUTHOR'] and ['CLASSIFY']. This is raw data.\n",
    "\n",
    "        vectorizer:cofig\n",
    "            When we apply to \n",
    "\n",
    "        Return:\n",
    "        -------\n",
    "        X_vector:array\n",
    "            Dimension = (cleaned example, MAX_WORD).Cleaned data and transformed to TFIDF format \n",
    "            with original sequence, which means can be matched withcooresponding y_labels.\n",
    "\n",
    "        y_vector:\n",
    "            Dimension = (no. cleaned examples, NUM_LABELS)=(9630,3)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"*\"*50,\"Start TFIDF transfrom\", \"*\"*50)\n",
    "        start_time = time.time()\n",
    "\n",
    "         #*****************************cut down Part of optimization******************************************\n",
    "    #     #we can't directly drop column because index will dynamicly decrease, but we can gather index of drop\n",
    "    #     index_drop=[]\n",
    "    #     #iteration through all DataFrame\n",
    "    #     for i in range(len(df)):\n",
    "    #         #according to plot and statitc result,>600 have 2628, <60 have 3540, so we only need 60<data<600\n",
    "    #         if len(df.iloc[i,0])<60 or len(df.iloc[i,0])>600:\n",
    "    #             #delete too big and too small\n",
    "    #             index_drop.append(i)\n",
    "    #     print(f\"before drop shape={df.shape}\")\n",
    "\n",
    "    #     #drop row by list but remain old index number\n",
    "    #     df = df.drop(index_drop,axis=0)\n",
    "    #     print(f\"after drop shape={df.shape}\")\n",
    "    #     #re.finall(\\w+)  \n",
    "    #     #*****************************cut down Part of optimization******************************************\n",
    "\n",
    "        # inintial we need use vectorizer from train to process test data\n",
    "        if vectorizer == None:\n",
    "            vectorizer = TfidfVectorizer(sMAX_WORD='english', max_features = MAX_WORD)\n",
    "            #when we built a vect, we need to import all words as corpus combination to generate a TFIDF dictornary\n",
    "            vect = vectorizer.fit([\" \".join(df['AUTHOR'].tolist())])\n",
    "            #after fit, we need to use dictionary to transform our coupus into TFIDF(we can use sum() to check sparse)\n",
    "            data_2 = vect.transform(df['AUTHOR']).toarray()\n",
    "            X_vector = np.array(data_2)\n",
    "        else:\n",
    "            vect = vectorizer.fit([\" \".join(df['AUTHOR'].tolist())])\n",
    "            #after fit, we need to use dictionary to transform our coupus into TFIDF(we can use sum() to check sparse)\n",
    "            X_vector = vect.transform(df['AUTHOR']).toarray()\n",
    "\n",
    "        if to_categorical == True:\n",
    "            #To remain the relation between AUTHOR vector and CLASSIFY labels, we also output y_vector coorespondingly\n",
    "            y_vector = to_categorical(np.array(df['CLASSIFY']))\n",
    "        else:\n",
    "            y_vector = df['CLASSIFY']\n",
    "\n",
    "        cost_time = round((time.time()- start_time),4)\n",
    "        print(\"*\"*40,\"End tfidf() with {} second\".format(cost_time), \"*\"*40, end='\\n\\n')\n",
    "\n",
    "        return df, X_vector, y_vector, vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that more robust if i divide 13332 for every value in vector dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot\n",
    "BLUE Rougue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_label_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6.8 Split Data\"></a>\n",
    "### 6.8 Split Data\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X_vector,y_vector, test_size=0.2):\n",
    "    \"\"\"\n",
    "    this is only for padded data split\n",
    "    \"\"\"\n",
    "    print(\"*\"*50,\"Start train_test_split\", \"*\"*50)\n",
    "    start_time = time.time()\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_vector, y_vector, \\\n",
    "                                                        test_size=0.2, random_state=37)\n",
    "    \n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 37)\n",
    "    \n",
    "    cost_time = round((time.time()- start_time),4)\n",
    "    print(\"*\"*40,\"End embedding() with {} seconds\".format(cost_time), \"*\"*40, end='\\n\\n')\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, X_val, y_train, y_test, y_val = split_data(question_padded,y_label_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7.Classify\"></a>\n",
    "# 7.Classify\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will provide more than three type of input. All matrix will use number represent a whole sentence. What we can do include surpivsed learning to classify (NN, KNN) and unsurprivised learning to cluster (k-means + PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7.1 KNN\"></a>\n",
    "### 7.1 KNN\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7.2 Neural Network Classify\"></a>\n",
    "### 7.2 Neural Network Classify\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tranform category to one-hot, last layer use softmax, use argmax() to get predict result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7.3 K-means cluster + PCA\"></a>\n",
    "### 7.3 K-means cluster + PCA\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get high dimentsion cluster result and reduce dimenstion to get a 3D/2D visulization result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8.Models\"></a>\n",
    "# 8.Models\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(word_index):\n",
    "    \"\"\"\n",
    "    Input is just padded question sequence, add Embedding layer transorfrom it into word vector and build up a sentence\n",
    "    \"\"\"\n",
    "    #*************Hyperparameters************\n",
    "    #max sequence/sentence length\n",
    "    MAX_SEQ_LEN = 300\n",
    "    # what is the word dimentsion for a single, for examplee, \"thank\" will have \n",
    "    EMBEDDING_DIM = 100\n",
    "    #*************Hyperparameters************\n",
    "    model = None\n",
    "    \n",
    "    \n",
    "    input_layer_1 = Input(shape=(MAX_SEQ_LEN,), dtype='int32')\n",
    "    embed_layer_2 = Embedding(input_dim = len(word_index) + 1, \\\n",
    "                              output_dim = EMBEDDING_DIM, \\\n",
    "                              input_length=MAX_SEQ_LEN\n",
    "                              )(input_layer_1)\n",
    "    pooling_layer_3 = GlobalAveragePooling1D()(embed_layer_2)\n",
    "    dense_layer_4 = Dense(units = 32, activation='relu')(pooling_layer_3)\n",
    "    output_layer_5 = Dense(units = 4)(dense_layer_4)\n",
    "    model = Model(inputs = input_layer_1, outputs = output_layer_5, name='nn_model')\n",
    "    model.summary()\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-25df84ba6728>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnn_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'word_index' is not defined"
     ]
    }
   ],
   "source": [
    "nn_model(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.CNN(u-gim, bi, trigum = contcant)\n",
    "2.CNN \n",
    "3.GRU\n",
    "4.BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_fit(model_input, X_train, X_val, y_train, y_val, loss_fun, epoch_num=3):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"*\"*40,\"Start {} Processing\".format(model_input._name), \"*\"*40)\n",
    "    \n",
    "    model = model_input\n",
    "#     METRICS = [\n",
    "#           metrics.TruePositives(name='tp'),\n",
    "#           metrics.FalsePositives(name='fp'),\n",
    "#           metrics.TrueNegatives(name='tn'),\n",
    "#           metrics.FalseNegatives(name='fn'), \n",
    "#           metrics.CategoricalAccuracy(name='accuracy'),\n",
    "#           metrics.Precision(name='precision'),\n",
    "#           metrics.Recall(name='recall'),\n",
    "#           metrics.AUC(name='auc'),\n",
    "#           F1Score(num_classes = int(y_train.shape[1]), name='F1')\n",
    "#     ]\n",
    "\n",
    "    learning_rate = 1e-2\n",
    "    opt_adam = optimizers.Adam(lr = learning_rate, decay=1e-5)\n",
    "    model.compile(loss=loss_fun, \\\n",
    "                  optimizer=opt_adam, \\\n",
    "                  metrics = ['mae','mse'])\n",
    "    # batch_size is subjected to my GPU and GPU memory, after testing, 32 is reasonable value size. \n",
    "    # If vector bigger, this value should dercrease\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), \\\n",
    "                        epochs=epoch_num, batch_size=32, verbose =1)\n",
    "    # dic = ['loss', 'accuracy', 'val_loss','val_accuracy']\n",
    "    history_dict = [x for x in history.history]\n",
    "    # model.predict(train_features[:10])\n",
    "\n",
    "#     print(\"*\"*50)\n",
    "#     x_axis = list(range(epoch_num))\n",
    "#     # loss\n",
    "#     plt.plot(x_axis, history.history[history_dict[0]], color = 'r', lw = 2, label = history_dict[0])\n",
    "#     # val_loss\n",
    "#     plt.plot(x_axis, history.history[history_dict[10]], color = 'y', lw = 2, label = history_dict[10])\n",
    "#     # accuracy\n",
    "#     plt.plot(x_axis, history.history[history_dict[5]], color = 'b', lw = 2, label = history_dict[5])\n",
    "#     # validataion_accuracy\n",
    "#     plt.plot(x_axis, history.history[history_dict[15]], color = 'k', lw = 2, label = history_dict[15])\n",
    "#     plt.title(model_input._name)\n",
    "#     plt.legend()\n",
    "#     plt.xlabel('Epochs')\n",
    "#     # plt.ylabel(str(dic[i]))\n",
    "#     plt.show()\n",
    "    \n",
    "    cost_time = round((time.time()- start_time),4)\n",
    "    print(\"*\"*40,\"End {} with {} seconds\".format(model_input._name, cost_time), \"*\"*40, end='\\n\\n')\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4863, 300)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"nn_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        [(None, 300)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 300, 100)          7207100   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_4 ( (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 32)                3232      \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 7,210,464\n",
      "Trainable params: 7,210,464\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "**************************************** Start nn_model Processing ****************************************\n",
      "Train on 4863 samples, validate on 1216 samples\n",
      "Epoch 1/10\n",
      "4863/4863 [==============================] - 12s 2ms/sample - loss: 0.0647 - mae: 0.1896 - mse: 0.0647 - val_loss: 0.0490 - val_mae: 0.1636 - val_mse: 0.0490\n",
      "Epoch 2/10\n",
      "4863/4863 [==============================] - 11s 2ms/sample - loss: 0.0435 - mae: 0.1563 - mse: 0.0435 - val_loss: 0.0529 - val_mae: 0.1657 - val_mse: 0.0529\n",
      "Epoch 3/10\n",
      "4863/4863 [==============================] - 11s 2ms/sample - loss: 0.0367 - mae: 0.1405 - mse: 0.0367 - val_loss: 0.0506 - val_mae: 0.1649 - val_mse: 0.0506\n",
      "Epoch 4/10\n",
      "4863/4863 [==============================] - 11s 2ms/sample - loss: 0.0316 - mae: 0.1284 - mse: 0.0316 - val_loss: 0.0539 - val_mae: 0.1709 - val_mse: 0.0539\n",
      "Epoch 5/10\n",
      "4863/4863 [==============================] - 11s 2ms/sample - loss: 0.0282 - mae: 0.1195 - mse: 0.0282 - val_loss: 0.0597 - val_mae: 0.1797 - val_mse: 0.0597\n",
      "Epoch 6/10\n",
      "4863/4863 [==============================] - 11s 2ms/sample - loss: 0.0256 - mae: 0.1127 - mse: 0.0256 - val_loss: 0.0569 - val_mae: 0.1683 - val_mse: 0.0569\n",
      "Epoch 7/10\n",
      "4863/4863 [==============================] - 11s 2ms/sample - loss: 0.0249 - mae: 0.1097 - mse: 0.0249 - val_loss: 0.0542 - val_mae: 0.1642 - val_mse: 0.0542\n",
      "Epoch 8/10\n",
      "4863/4863 [==============================] - 11s 2ms/sample - loss: 0.0226 - mae: 0.1042 - mse: 0.0226 - val_loss: 0.0565 - val_mae: 0.1712 - val_mse: 0.0565\n",
      "Epoch 9/10\n",
      "4863/4863 [==============================] - 11s 2ms/sample - loss: 0.0213 - mae: 0.0999 - mse: 0.0213 - val_loss: 0.0612 - val_mae: 0.1762 - val_mse: 0.0612\n",
      "Epoch 10/10\n",
      "4863/4863 [==============================] - 11s 2ms/sample - loss: 0.0203 - mae: 0.0967 - mse: 0.0203 - val_loss: 0.0633 - val_mae: 0.1763 - val_mse: 0.0633\n",
      "**************************************** End nn_model with 112.0545 seconds ****************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# history, model_2 = compile_fit(nn_model(), X_train, X_test, X_val, y_train, y_test, y_val, loss_fun = 'binary_crossentropy', epoch_num=50)\n",
    "history, model_2 = compile_fit(nn_model(word_index), X_train, X_val, y_train, y_val, loss_fun = 'mse', epoch_num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9.Main Function\"></a>\n",
    "# 9.Main Function\n",
    "<a href=\"#2.Table of Contents\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_function():\n",
    "    \"\"\"\n",
    "    We use this function to call process one by one.\n",
    "    \"\"\"\n",
    "    pre = preprocess_data()\n",
    "    df_train_raw, X_question_df, X_answer_df, y_question_df, y_answer_df = pre.import_data(\"03_data/02_train.csv\")\n",
    "#     df_test_raw, X_q_test_df, X_a_test_df, y_q_test_df, y_a_test_df = pre.import_data(\"03_data/03_test.csv\")\n",
    "\n",
    "    clean = clean_data()\n",
    "    question_cleaned_df = clean.clean_process(X_question_df, column_1 ='question')\n",
    "    answer_cleaned_df = clean.clean_process(X_answer_df, column_1='answer')\n",
    "#     q_test_cleaned_df = clean.clean_process(X_q_test_df, column_1 ='question')\n",
    "#     a_test_cleaned_df = clean.clean_process(X_a_test_df, column_1='answer')\n",
    "    \n",
    "    eda_class = eda_data()\n",
    "    # eda_class.question_plot(y_question_df)\n",
    "    # question_padded have shape (6079,100) can be used in fewer embedding\n",
    "    question_padded, question_cleaned_df, word_index = eda_class.tokenize_plot(question_cleaned_df, answer_cleaned_df)\n",
    "    # get question label\n",
    "    y_label_df = eda_class.label_feature(y_question_df)\n",
    "    \n",
    "    question_padded, question_cleaned_df, word_index = eda_class.tokenize_plot(question_cleaned_df, answer_cleaned_df)\n",
    "#     q_test_padded, q_test_cleaned_df = eda_class.tokenize_plot(q_test_cleaned_df, a_test_cleaned_df)\n",
    "#     # get question label\n",
    "#     y_label_test_df = eda_class.label_feature(y_q_test_df)\n",
    "    \n",
    "    \n",
    "    X_train, X_val, y_train, y_val = split_data(question_padded, y_label_df, test_size=0.2)\n",
    "    \n",
    "    history, model_2 = compile_fit(nn_model(word_index), X_train, X_val, y_train, y_val, loss_fun = 'mse', epoch_num=5)\n",
    "    \n",
    "    return X_question_df, X_answer_df, y_question_df, y_answer_df, \\\n",
    "            question_padded, question_cleaned_df, y_label_df,X_train, X_val, y_train, y_val, history,model_2, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** Start Clean data **************************************************\n",
      "**************************************** End clean_data() with 4.196 second ****************************************\n",
      "\n",
      "************************************************** Start Clean data **************************************************\n",
      "**************************************** End clean_data() with 4.1119 second ****************************************\n",
      "\n",
      "we got unique 72070 words\n",
      "we have 13332 words appear more than 5 times\n",
      "we got unique 72070 words\n",
      "we have 13332 words appear more than 5 times\n",
      "************************************************** Start train_test_split **************************************************\n",
      "**************************************** End embedding() with 0.0069 seconds ****************************************\n",
      "\n",
      "Model: \"nn_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 300)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 300, 100)          7207100   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                3232      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 7,210,464\n",
      "Trainable params: 7,210,464\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "**************************************** Start nn_model Processing ****************************************\n",
      "Train on 4863 samples, validate on 1216 samples\n",
      "Epoch 1/5\n",
      "4863/4863 [==============================] - 18s 4ms/sample - loss: 0.0600 - mae: 0.1823 - mse: 0.0600 - val_loss: 0.0485 - val_mae: 0.1665 - val_mse: 0.0485\n",
      "Epoch 2/5\n",
      "4863/4863 [==============================] - 17s 4ms/sample - loss: 0.0409 - mae: 0.1506 - mse: 0.0409 - val_loss: 0.0511 - val_mae: 0.1657 - val_mse: 0.0511\n",
      "Epoch 3/5\n",
      "2304/4863 [=============>................] - ETA: 8s - loss: 0.0323 - mae: 0.1304 - mse: 0.0323"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    X_question_df, X_answer_df, y_question_df, y_answer_df, \\\n",
    "    question_padded, question_cleaned_df, y_label_df,X_train, X_val, y_train, y_val, history,model_2,word_index  = main_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-0eb274559fbe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mword_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'word_index' is not defined"
     ]
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  33,   60, 2861,   41,   48, 1129, 4564,  212,    6, 1940,    1,\n",
       "        746,  237,   14, 1940, 3626,    1,  201, 5572,  767, 5188,  767,\n",
       "       3806,   15, 1435,  767, 2266, 1129, 4564,   35,   40,    3,   47,\n",
       "        709,   14,   12,    2,  521,   14,    2, 1963,   95,    5,    8,\n",
       "       1115,    5, 1299,    4, 2415,  413,    5, 3887,   28,  202,   12,\n",
       "        953,   25,  700,    3,  122, 4301,  201, 2012, 8960,   94,   18,\n",
       "       2239,    5, 5189,   55,    3,   13,  175,    3, 3389,  760, 8960,\n",
       "        731,    8,   11,   12, 8961,    4,    1, 2415,   31,   13,    6,\n",
       "        482,  142,   27,   39,  954,   22, 1274,  280,    5, 1940,  767,\n",
       "        151, 5573, 1940,  121,   60,   20,  149,  931,    7,  437,  125,\n",
       "       1230,  767,   26, 1044,    5,    2, 2064, 1129, 4564,  839,   11,\n",
       "       1054, 5025,  694,   33,   60, 2861,   41,   48, 4564, 5378,   14,\n",
       "        347,  767,  151,  212,    6, 1940,  767,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_padded[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-a7a0131e6510>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mplot_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_history(history):\n",
    "  hist = pd.DataFrame(history.history)\n",
    "  hist['epoch'] = history.epoch\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Abs Error [MPG]')\n",
    "  plt.plot(hist['epoch'], hist['mae'],\n",
    "           label='Train Error')\n",
    "  plt.plot(hist['epoch'], hist['val_mae'],\n",
    "           label = 'Val Error')\n",
    "  plt.ylim([0,0.3])\n",
    "  plt.legend()\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Square Error [$MPG^2$]')\n",
    "  plt.plot(hist['epoch'], hist['mse'],\n",
    "           label='Train Error')\n",
    "  plt.plot(hist['epoch'], hist['val_mse'],\n",
    "           label = 'Val Error')\n",
    "  plt.ylim([0, 0.2])\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1216/1216 [==============================] - 0s 337us/sample - loss: 0.0654 - mae: 0.1780 - mse: 0.0654\n",
      "Testing set Mean Abs Error:  0.18\n"
     ]
    }
   ],
   "source": [
    "loss, mae, mse = model_2.evaluate(X_val, y_val, verbose=1)\n",
    "\n",
    "print(\"Testing set Mean Abs Error: {:5.2f}\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = model_2.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1216"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_2[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1216"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_val.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list(y_val.iloc[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_predictions = model_2.predict(X_val)\n",
    "\n",
    "plt.scatter(x = y_val.iloc[:,2].values.flatten(), y = test_predictions[:,2].flatten())\n",
    "plt.xlabel('True Values [MPG]')\n",
    "plt.ylabel('Predictions [MPG]')\n",
    "plt.axis('equal')\n",
    "plt.axis('square')\n",
    "plt.xlim([0,plt.xlim()[1]])\n",
    "plt.ylim([0,plt.ylim()[1]])\n",
    "_ = plt.plot([-100, 100], [-100, 100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "error = test_predictions[:,2].flatten() - y_val.iloc[:,2].values.flatten()\n",
    "plt.hist(error, bins = 25)\n",
    "plt.xlabel(\"Prediction Error [MPG]\")\n",
    "_ = plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  115,   433,   400, ...,     0,     0,     0],\n",
       "       [12073,     8,     1, ...,     0,     0,     0],\n",
       "       [  780,   407,  1193, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [   34,     3,   354, ...,     0,     0,     0],\n",
       "       [   21,  2153,   204, ...,     0,     0,     0],\n",
       "       [ 3589,   393,  1795, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_label_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e6c85e1aec0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_label_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_label_df' is not defined"
     ]
    }
   ],
   "source": [
    "y_label_df.loc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"10.Unused Code\"></a>\n",
    "# 10.Unused Code\n",
    "<a href=\"#1.Summary\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.using question_user_page as benchmark, split quetsion_title or question_body into train_test, we believe different website have different type questions, so we can make evaluate and predict model\n",
    "\n",
    "2.Using unsupervise learning to cluster question into differnt type, culster is depending on data preprocessing granularity. smaller grandularity, more spase cluster\n",
    "\n",
    "3.After i embedding these sentence, you can use KNN SVM to do unsuperviese cluster\n",
    "\n",
    "4.Using categore to cluster by CNN(n-gram / Glove / miniGPT）\n",
    "\n",
    "5.Generage numerical value by former data and compart to anser_well_written\n",
    "\n",
    "6.extract the root url like photo.stackchange.com to try to classfiy it with some argothrim, same question is to catgory column\n",
    "\n",
    "7.If the result is not good enough, try to use url to grab more data to analysis\n",
    "\n",
    "8.The data for this competition includes questions and answers from various StackExchange properties. Your task is to predict target values of 30 labels for each question-answer pair.\n",
    "\n",
    "The list of 30 target labels are the same as the column names in the sample_submission.csv file. Target labels with the prefix question_ relate to the question_title and/or question_body features in the data. Target labels with the prefix answer_ relate to the answer feature.\n",
    "\n",
    "9.for each dataframe maybe we need add category, and that will imporove performance\n",
    "\n",
    "10.Stopword is meaningful for answer sequence, and so as punctuation. Try to only eliminate useless punctuatinon like '\\`' but remain '?'and '!'\n",
    "11.embedding is random initial word vector, but we can use Glove to import pretrain to impove performance\n",
    "\n",
    "12.evalution part try to use BLEU score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.tensorflow.org/tutorials/text/word_embeddings?hl=zh-cn\n",
    "- https://towardsdatascience.com/nlp-building-a-question-answering-model-ed0529a68c54"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
